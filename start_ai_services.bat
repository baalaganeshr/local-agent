@echo off
REM =================================================================
REM   AI AGENTS STARTUP SCRIPT FOR WINDOWS
REM   Complete integration with single chat interface
REM =================================================================

echo.
echo ğŸš€ STARTING AI AGENTS INTEGRATION SYSTEM
echo =========================================
echo.

REM Set Ollama models to G drive (more space)
set OLLAMA_MODELS=G:\ollama_models
echo ğŸ“ Setting Ollama models location to: %OLLAMA_MODELS%

REM Navigate to project directory
cd /d "G:\c\OneDrive\Desktop\localai\local-agent"
echo ğŸ“‚ Project directory: %CD%

REM Check if virtual environment exists
if not exist "venv\Scripts\python.exe" (
    echo âŒ Virtual environment not found!
    echo Please create one with: python -m venv venv
    pause
    exit /b 1
)

echo.
echo ğŸ”§ STARTING SERVICES...
echo.

REM 1. Start Ollama (if not running)
echo ğŸ¤– Starting Ollama service...
tasklist /fi "imagename eq ollama.exe" 2>nul | find /i "ollama.exe" >nul
if %errorlevel% neq 0 (
    start "" ollama serve
    timeout /t 5 >nul
) else (
    echo    âœ… Ollama already running
)

REM 2. Start Local AI Service (Port 8001)
echo ğŸ§  Starting Local AI Service on port 8001...
start "Local AI Service" /min .\venv\Scripts\python.exe local-ai-service\start_api_gdrive.py

REM 3. Start Unified Agent API (Port 8002)
echo ğŸ”— Starting Unified Agent API on port 8002...
start "Unified API" /min .\venv\Scripts\python.exe unified_agent_api.py

REM 4. Start Marketplace Service (Port 8080)
echo ğŸ›ï¸ Starting Marketplace Service on port 8080...
start "Marketplace" /min .\venv\Scripts\python.exe zero-cost-ai-marketplace\simple_launcher.py

echo.
echo â³ Waiting for services to initialize...
timeout /t 10 >nul

REM 5. Verify all services are running
echo.
echo ğŸ” CHECKING SERVICE STATUS...
echo.

netstat -an | findstr ":11434" >nul && echo    âœ… Ollama: RUNNING (Port 11434) || echo    âŒ Ollama: NOT RUNNING
netstat -an | findstr ":8001" >nul && echo    âœ… Local AI: RUNNING (Port 8001) || echo    âŒ Local AI: NOT RUNNING
netstat -an | findstr ":8002" >nul && echo    âœ… Unified API: RUNNING (Port 8002) || echo    âŒ Unified API: NOT RUNNING
netstat -an | findstr ":8080" >nul && echo    âœ… Marketplace: RUNNING (Port 8080) || echo    âŒ Marketplace: NOT RUNNING

echo.
echo ğŸŒ OPENING CHAT INTERFACE...
echo.

REM 6. Open the chat interface in default browser
start "" "local-ai-service\ai_chat.html"

echo.
echo ============================================
echo ğŸ‰ AI AGENTS SYSTEM READY!
echo ============================================
echo.
echo ğŸ“± Chat Interface: Opened in your browser
echo ğŸ¤– Available Models: llama3.2:1b (1.3GB)
echo ğŸ’¾ Model Location: G:\ollama_models
echo ğŸ”§ All services integrated and running
echo.
echo ğŸ“‹ Service URLs:
echo    â€¢ Chat Interface: file:///.../ai_chat.html
echo    â€¢ Unified API: http://localhost:8002
echo    â€¢ Local AI API: http://localhost:8001  
echo    â€¢ Marketplace: http://localhost:8080
echo    â€¢ Ollama: http://localhost:11434
echo.
echo ğŸ’¡ Usage:
echo    1. Use the opened chat interface
echo    2. Upload files with the + button
echo    3. Chat with your local AI models
echo.
echo âš ï¸  To stop all services, run: stop_ai_services.bat
echo.
pause
